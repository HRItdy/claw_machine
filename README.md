# claw_machine

## This repo is a project to control the UR5 robot to grasp object. 
**Model used:** 
GPT, GroundingDINO, Segment-Anything

**Features:** 
1. The **GroundingDINO** and **Segment-Anything** are integrated to detect the object.
2. The functions are wrapped as individual services. 

**TODO:**
- Enhance the groundingdino with GPT-4.
- Now the detection is only executed once. If want to track the object, can use XMEM or recent SAM-v2.
- Write the launch to run the nodes simultaneously.
- Form a document.


## Sturcture:

**LargestCluster:** A nodelet to select the largest cluster from the clusters generated by EuclideanCluster nodelet.

**PickUp:** The main package including detection, depth and manipulation services.
- pc_calibration.py:
- models.py: logic to load GroundingDINO and Segment-Anything. **TODO:** change the path to relative path.
- claw_detect.py
- claw_depth.py
- ur_executor.py
- claw_pickup.py
- **external**: folder containing the call_service script can be called in other workpackages (called in PromptChat project).


## How to use:
### Run sys_lux.launch
### Run pc_transform.py

### Run pc_calibration.py
 - For image point input, run `python pc_calibration.py`, select four points in counterclockwise order. The order of the points is indicated by number. Remember to input the pointcloud coordinates accordingly.
 - For point cloud points input, run `rostopic echo /clicked_point`, select `Publish point` in rviz, click the bottom of each ball, the coordinates will be published into `/clicked_point` topic. Input them into the terminal.
### Run claw_detection.py and call_detect_service.py

### Run pc_segment.py
The center point is published to `..... ` TODO: Add the pipeline diagram and the architecture of the topics in readme.
