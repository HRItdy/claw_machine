# claw_machine

## This repo is a project to control the UR5 robot to grasp object. 
**Model used:** 
GPT, GroundingDINO, Segment-Anything

**Features:** 
1. The **GroundingDINO** and **Segment-Anything** are integrated to detect the object.
2. The functions are wrapped as individual services. 

**TODO:**
- Enhance the groundingdino with GPT-4.
- Now the detection is only executed once. If want to track the object, can use XMEM or recent SAM-v2.
- Write the launch to run the nodes simultaneously.
- Form a document.


## Sturcture:

**LargestCluster:** A nodelet to select the largest cluster from the clusters generated by EuclideanCluster nodelet.

**PickUp:** The main function. Now contains the detection and grasping. Overlay is to be done.

**call_service_global.py:** The client to call the detection service. Can be placed in any other ROS package or projects. But remember to place the `srv` file into the project.

## How to use:
### Run sys_lux.launch
### Run pc_transform.py

### Run pc_calibration.py
 - For image point input, run `python pc_calibration.py`, select four points in counterclockwise order. The order of the points is indicated by number. Remember to input the pointcloud coordinates accordingly.
 - For point cloud points input, run `rostopic echo /clicked_point`, select `Publish point` in rviz, click the bottom of each ball, the coordinates will be published into `/clicked_point` topic. Input them into the terminal.
### Run claw_detection.py and call_detect_service.py

### Run pc_segment.py
The center point is published to `..... ` TODO: Add the pipeline diagram and the architecture of the topics in readme.
